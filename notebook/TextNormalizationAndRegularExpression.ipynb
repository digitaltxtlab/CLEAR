{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note book is following the structure of the chapter 2 of the book \"*An Introduction to Natural Language Processing,\n",
    "Computational Linguistics, and Speech Recognition*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce the basic techniques to do preprocessing or **text normalization**. In general, text normalization is a set of procedures to convert text to a more convenient, standard form.Here we will include **tokenization**,**lemmatization** and **stemming** , also we will introduce the fundamental tool in language processing - **regular expression**. \n",
    "\n",
    "**Tokenization** refers to the task of separating the text word by word, most of the latin language can be separated by \"white space\", but the sometimes it is required to treat things differently, for example, we often treat \"New York\" as a single word.\n",
    "\n",
    "**Lemmatization** and **stemming** are closely related, lemmatization will map all the different of a words to its root, for example, sang, sung, and sing will all be mapped to verb sing. Stemming is a simpler version of the lemmatization, it only strip the suffix from the end of the words.\n",
    "\n",
    "We divide each topic into several sub-sections,and each section will have an example coming after the definition.\n",
    "\n",
    "Let's dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression is a useful tool when doing text normalization， it is a language for specifying the strings to be searched. Many string processing functions in python support the use of regular expression, here we use *re.search* as an example to show how the regular expression works, in the end of this section, we will introduce more functions which could use regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #this the library for using rugular expression in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Basic Regular Expression Patterns\n",
    "The simplest regular expression is to match a sequence of simple characters. For example, suppose we have the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"Though hundreds of thousands and Thousands had done their very best to disfigure the small piece of land on which they were crowded together\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to match the words \"thousands\" then we will simply use /thousand/ to match the words, *findall* function will check for a match anywhere in the text, then return all the matched string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchObj = re.findall(r'thousands',Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thousands']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Regular 1.1**： **[ ]**，**^**， and **-** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression may not seem useful at this point, but suppose now we want to match the both 'thousands' and 'Thousands', this is where \"[]\" comes to play a role - the regular expression will match both string of characters inside the braces. Now we have [Tt]housands to match both **thousands** and **Thousands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchObj = re.findall(r'[Tt]housands',Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thousands', 'Thousands']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things become interesting when we have this *[ ]* tool, we can match all the single digit using [1234567890], and use [ABCDEFGHIJKLMNOPQRSTUVWXYZ] to match any capital letter, let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"An apple falls from the tree, there are 2 Birds and 3 Monkeys on the tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchObj = re.findall(r'[1234567890]',Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchObj = re.findall(r'[ABCDEFGHIJKLMNOPQRSTUVWXYZ]',Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'M']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's continue to expore some other tools which come along with *[ ]*, using them can help express richer structure.The brackets can be used with the dash (-) to specify range any one character in a range，for example, the pattern *[1-9]* specify any one of the characters from 1 to 9. And the patttern *[A-Z]* is equivalent to the expression we used abouv - *[ABCDEFGHIJKLMNOPQRSTUVWXYZ]*. Another tool coming along with *[ ]* is caret *^*, it represent negating when it is put to the first symbol after the open square brace. For example, *[^a-z]* means to match any single character except a,b,c,d,e... Let's see some example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"An apple falls from the tree, there are 2 Birds and 3 Monkeys on the tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchObj = re.findall(r'[1-9]',Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '3']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchObj = re.findall(r'[^a-z]',Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ',',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '2',\n",
       " ' ',\n",
       " 'B',\n",
       " ' ',\n",
       " ' ',\n",
       " '3',\n",
       " ' ',\n",
       " 'M',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Regular 1.2**: **?**, *****, **+**,and **.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*?* is another common symbol we use, it represent the \"the preceding character or nothing\". For example, *falls?* will match both \"fall\" and \"falls\". See an example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"An apple falls from the tree while children fall from another tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchObj = re.findall(r'falls?',Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['falls', 'fall']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the question mark *?* is matching \"zeros or one instances of the previous character\", we have asterisk \\* to match the \"zero or more occurrences of the immediately previous character\". For example, \"ab\\*\" will match all the following text: \"ab\", \"abb\", \"abbbb\". Interestingly, we can use [0-9][0-9]\\* to match any integer. Another symbol *+* has almost the same function as \\*, only with one difference: it only match \"**one** or more occurrences of the immediately previous character\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', 'abb', 'abbbb', 'abbbb', 'a']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text = \"Some dummy words； ab, abb, abbbb, abbbb, a\"\n",
    "matchObj = re.findall(r'ab*',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use \"ab+\" instead of \"ab*\", it will only match the \"ab\", \"abb\", \"abbb...\", but not \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', 'abb', 'abbbb', 'abbbb']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj = re.findall(r'ab+',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['57', '2']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text = \"There are 57 aplles and 2 birds on the tree\"\n",
    "matchObj = re.findall(r'[0-9][0-9]*',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Period **.** is another very important expression, which is used to match any single character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['began', 'begin']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text = \"began is the past tense of begin\"\n",
    "matchObj = re.findall(r'beg.n',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Regular 1.3:** Anchors\n",
    "\n",
    "Anchors are special characters that anchor regular expressions to particular places in a string. The common used anchors are caret *^* and dollar sign *$*. Caret *^* is used to match the start of a line, so *^began* will only match the word \"began\" at the start of a line. Similarly, \\$ is used to match the things at the end of a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text = \"end1 of the a period of life often indicates the beginning of another journey,so end2ing is not always the end3\"\n",
    "matchObj = re.findall(r'^end.',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end1', 'end2', 'end3']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj = re.findall(r'end.',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end3']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj = re.findall(r'end.$',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Disjunction, Grouping and Precedence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1: ** Disjunction\n",
    "\n",
    "Disjunction operator is also called the pipe symbol \"|\", is similar to the \"or\" in our natural language. Suppose we want to search for \"bird or monkey\" in the sentence, how can we do this? Someone may think of using *[ ]* to do this, but the problem is we can only apply the bracket on single character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bird', 'monkeys']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text = \"Here are a bird and some monkeys on the tree\"\n",
    "matchObj = re.findall(r'bird|monkeys',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2: **Precedence and Grouping\n",
    "\n",
    "The parenthesis symbol - \"()\"is used for both Precedence and Grouping. Suppose you want to match both \"study\" and \"studies\", in this case, we may want to use disjunction operator, but \"study|ies\" will not match study and studies at the same time, instead, it will match \"study\" and \"ies\". This is because by default, the sequence like \"study\" and \"ies\" take precedence over the disjunction operator \"|\". To solve this probelm, we can use  \"()\" to specify the precedence as we want. So \"stud(y)|(ies)\" will do what we want.\n",
    "\n",
    "In general, adding the parenthesis around the text is helping us to group things, but remember, the grouping is realized by changing the precedence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study', 'ies']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text = \"Tom like to study history, while his brother studies maths in the college\"\n",
    "matchObj = re.findall(r'study|ies',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*?:* is called non-capturing group,*()* have two meanning: one is specify the precedence, and the other is to actually store the things in the bracket in the computer and output it when necessary.\n",
    "\n",
    "To differentiate these two cases: we have *()*, which will both grouping things and store the group in the computer,and *(?:)* will merely specify the preference.\n",
    "\n",
    "Basically, if we merely put *()* here, findall fucntion will only return the content in the braket, in this case, only return \"y,ies\", sometimes we may need to use *()* because we only want to match some pattern but only return part of words within this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study', 'studies']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj = re.findall(r'stud(?:y|ies)',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y', 'ies']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj = re.findall(r'stud(y|ies)',Text)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: An example to show the use of Regular Expresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example showing how we can construct a regular expression step by step, this resulting expression will help us to match what we want to match. I extract an article about the history of England, suppose now we want to have a list of times (years) occurred in the article, and we need to write the regular expression to help us to finish this task. Here is how this passage look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('History of England(used for regex tutorial).txt', 'rb') as myfile:\n",
    "    data=myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'History of England\\r\\nFrom Wikipedia, the free encyclopedia\\r\\n\\r\\nEngland became inhabited more than 800,000 years ago, as the discovery of stone tools and footprints at Happisburgh in Norfolk has revealed.[1] The earliest evidence for early modern humans in North West Europe, a jawbone discovered in Devon at Kents Cavern in 1927, was re-dated in 2011 to between 41,000 and 44,000 years old.[2] Continuous human habitation in England dates to around 13,000 years ago (see Creswellian), at the end of the'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.decode(\"utf-8\")\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the text, we can see that there are three \"types\" of ways to mention years: First one is the most common one, it just write: \"In 2011\", where 2011 can be replaced by any number; second way is \"4000 BC/AD\", where \"4000\" can be replaced by any other number; and the third way is some expression like this \"1495–1497\", it refers to a certain period. We will write the regular expresion to capture this three formats in the following:\n",
    "\n",
    "It is straight forward to write down the expression using what we have learnt from previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in 1927',\n",
       " 'in 2011',\n",
       " 'In 1066',\n",
       " 'in 1485',\n",
       " 'in 1660',\n",
       " 'in 1707',\n",
       " 'In 55',\n",
       " 'In 2003',\n",
       " 'in 43',\n",
       " 'in 60']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj = re.findall(r'[Ii]n [0-9]+',data)\n",
    "matchObj[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000 BC',\n",
       " '6500 BC',\n",
       " '9000 BC',\n",
       " '4000 BC',\n",
       " '3806 BC',\n",
       " '2500 BC',\n",
       " '800 BC',\n",
       " '400 BC',\n",
       " '400 BC',\n",
       " '325 BC',\n",
       " '150 BC',\n",
       " '300 BC',\n",
       " '100 BC',\n",
       " '54 BC',\n",
       " '43 AD',\n",
       " '54 AD',\n",
       " '60 AD',\n",
       " '138 AD',\n",
       " '700 AD']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj = re.findall(r'[0-9]+ (?:BC|AD)',data)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait for a minute, if you look at the results carefully, you may find something weird, what does \"000 BC\" mean? It turned out some of the year is expressed using in this format: \"9,000 BC\", so there would be comma inserted in the numbers somewhere. And instead of using \"[0-9]+\", which match the normal integer, we will add \"(,[0-9]{3})\\*\" to match the integers containing the comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9,000 BC',\n",
       " '6500 BC',\n",
       " '9000 BC',\n",
       " '4000 BC',\n",
       " '3806 BC',\n",
       " '2500 BC',\n",
       " '800 BC',\n",
       " '400 BC',\n",
       " '400 BC',\n",
       " '325 BC',\n",
       " '150 BC',\n",
       " '300 BC',\n",
       " '100 BC',\n",
       " '54 BC',\n",
       " '43 AD',\n",
       " '54 AD',\n",
       " '60 AD',\n",
       " '138 AD',\n",
       " '700 AD']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj = re.findall(r'[0-9]+(?:,[0-9]{3})* (?:BC|AD)',data)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1135–1154',\n",
       " '1337–1453',\n",
       " '1649–1653',\n",
       " '1653–1659',\n",
       " '18th–19th',\n",
       " '3807–3806',\n",
       " '600–400',\n",
       " '150–100',\n",
       " '1139–1153',\n",
       " '1216–1272',\n",
       " '1272–1307',\n",
       " '1315–1317',\n",
       " '1327–1377',\n",
       " '1455–1485',\n",
       " '1470–1471',\n",
       " '1495–1497',\n",
       " '1516–1558',\n",
       " '1527–1598',\n",
       " '1558–1603',\n",
       " '1585–1604',\n",
       " '1585–1603',\n",
       " '1630–1660',\n",
       " '1642–1645',\n",
       " '18th–19th',\n",
       " '1832–1974',\n",
       " '1945–present',\n",
       " '1945–present',\n",
       " '1985–86']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchObj = re.findall(r'\\w+–\\w+',data)\n",
    "matchObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here \"\\w\" simply means the alphanumeric/underscore, which is equivalent to \"[a-zA-Z0-9_]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4: Summary of regular expression\n",
    "\n",
    "Construction of regular expression is a very time consuming and error-prone process, but it is extreme powerful because it is very fast to execute in the computer and can almost be used to match any pattern. Here we only have a very brief introduction to the regular expression, and a lot of thing hasn't been talked about. But this bit sould prepare you to understand the regular expression you encountered in other places, and when you want to use the regular expression yourself, it will be a good idea to search on the internet and see how other people construct the regular expression and have some inspiration to construct yours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization and Normalizaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is to segnment the running text into words, but just like other preprocessing tasks, word tokenization is highly dependent on the task we want to perform. For example, in some tasks, we want to keep the punctuation in our sentence and use it as a seperate token, but most of the time, if we only care about the the *semantic level* of the text (the meaning of the the text)，we can ignore the the punctuation and other non-alphanumeric characters in the text. Here I provide two functions: the first one can be used to get rid of the non-alphanumeric characters, the second can be used to tokenize the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "##removes non non-alphanumeric characters based on re\n",
    "def re_nalpha(str):\n",
    "    pattern = re.compile(r'[^\\w\\s]', re.U)\n",
    "    return re.sub(r'\\n','',re.sub(r'_', '', re.sub(pattern, '', str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text): return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the original text we have before the pre-processing is like following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"emmm... There is arc**haeological evidence of hu$man occupation of the Rome area from approximately 14,000 years ago, but the dense layer of much younger debris obscures Palaeolithic and Neolithic sites.[6] Evidence of stone tools, pottery, and stone weapons attest to about 10,000 years of human presence. Several excavations support the view that Rome grew from pastoral settlements on the Palatine Hill built above the area of the future Roman Forum. Between the end of the bronze age and the beginning of the Iron age, each hill between the sea and the Capitol was topped by a village (on the Capitol Hill, a village is attested since the end of the 14th century BC).[22]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the the *re_nalpha* to remove all the non-alphanumeric characters, and this function is actually using a very simple regular expression to match all the alphanumerical characters(represented by \\w) and space(represented by \\s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emmm There is archaeological evidence of human occupation of the Rome area from approximately 14000 years ago but the dense layer of much younger debris obscures Palaeolithic and Neolithic sites6 Evidence of stone tools pottery and stone weapons attest to about 10000 years of human presence Several excavations support the view that Rome grew from pastoral settlements on the Palatine Hill built above the area of the future Roman Forum Between the end of the bronze age and the beginning of the Iron age each hill between the sea and the Capitol was topped by a village on the Capitol Hill a village is attested since the end of the 14th century BC22'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = re_nalpha(data)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emmm',\n",
       " 'there',\n",
       " 'is',\n",
       " 'archaeological',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'human',\n",
       " 'occupation',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(alpha)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization in the English and other latin-origin languages are relatively easy, but when it comes to the other language like Chinese, tokenizaion is actually an challenging task. Also, there are some \"edge cases\" we haven't talk about, for example, how to deal with the \"what're\" case? should we seperate it into the \"what are\" or just a single string \"what're\"?  And should we regard \"New York\" as a single string or the two words? In these cases, we cannot merely rely on the space between the words to do tokenization. More advance algorithm will be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Collapsing words: Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the task of determining if two words have the same root, when we perform the lemmatization operation to text, all the words have the same root/origin will be mapped to the origin. For example, these three words \"am\",\"is\",\"are\" will be mapped to \"be\".\n",
    "\n",
    "Lemmatization will require very complexed algorithm, so in many cases, we will simply do a simpler version of lemmatization: stemming - just remove the suffix of the words. In this case, \"cats\" will become \"cat\", \"heights\" will become \"height\". Of course, not all the stemming is just to remove the \"s\" character at the end of the word, depending on the algorithm we used to do the stemming, it will have different stemming methods. Most widely used algorithm is proposed by Porter(1980). It will, for example, map \"accurate\" to \"accur\" \n",
    "\n",
    "Here let's see an example of stemming using the nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rome',\n",
       " 'area',\n",
       " 'from',\n",
       " 'approximately',\n",
       " '14000',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'but',\n",
       " 'the',\n",
       " 'dense',\n",
       " 'layer',\n",
       " 'of',\n",
       " 'much',\n",
       " 'younger',\n",
       " 'debris',\n",
       " 'obscures',\n",
       " 'palaeolithic',\n",
       " 'and',\n",
       " 'neolithic',\n",
       " 'sites6']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordList = word_tokenize(alpha)[10:30]\n",
    "wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rome\n",
      "area\n",
      "from\n",
      "approxim\n",
      "14000\n",
      "year\n",
      "ago\n",
      "but\n",
      "the\n",
      "dens\n",
      "layer\n",
      "of\n",
      "much\n",
      "younger\n",
      "debri\n",
      "obscur\n",
      "palaeolith\n",
      "and\n",
      "neolith\n",
      "sites6\n"
     ]
    }
   ],
   "source": [
    "for w in wordList:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
